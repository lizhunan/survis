const generatedBibEntries = {
    "00783": {
        "abstract": "We propose a 3D object detection method for autonomous driving by fully exploiting the sparse and dense, semantic and geometry information in stereo imagery. Our method, called Stereo R-CNN, extends Faster R-CNN for stereo inputs to simultaneously detect and associate object in left and right images. We add extra branches after stereo Region Proposal Network (RPN) to predict sparse keypoints, viewpoints, and object dimensions, which are combined with 2D left-right boxes to calculate a coarse 3D object bounding box. We then recover the accurate 3D bounding box by a region-based photometric alignment using left and right RoIs. Our method does not require depth input and 3D position supervision, however, outperforms all existing fully supervised image-based methods. Experiments on the challenging KITTI dataset show that our method outperforms the state-of-the-art stereo-based method by around 30 percent AP on both 3D detection and 3D localization tasks. Code will be made publicly available.",
        "author": "Peiliang Li and Xiaozhi Chen and Shaojie Shen",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/conf/cvpr/0001CS19.bib",
        "booktitle": "{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR 2019, Long Beach, CA, USA, June 16-20, 2019",
        "doi": "10.1109/CVPR.2019.00783",
        "keywords": "type: 3D object detection, binocular, Autonomous Driving",
        "pages": "7644--7652",
        "publisher": "Computer Vision Foundation / {IEEE}",
        "timestamp": "Mon, 30 Aug 2021 17:01:14 +0200",
        "title": "Stereo {R-CNN} Based 3D Object Detection for Autonomous Driving",
        "type": "inproceedings",
        "url": "http://openaccess.thecvf.com/content\\_CVPR\\_2019/html/Li\\_Stereo\\_R-CNN\\_Based\\_3D\\_Object\\_Detection\\_for\\_Autonomous\\_Driving\\_CVPR\\_2019\\_paper.html",
        "year": "2019"
    },
    "10992": {
        "abstract": "In this paper, we propose a novel 3D object detector that can exploit both LIDAR as well as cameras to perform very accurate localization. Towards this goal, we design an end-to-end learnable architecture that exploits continuous convolutions to fuse image and LIDAR feature maps at different levels of resolution. Our proposed continuous fusion layer encode both discrete-state image features as well as continuous geometric information. This enables us to design a novel, reliable and efficient end-to-end learnable 3D object detector based on multiple sensors. Our experimental evaluation on both KITTI as well as a large scale 3D object detection benchmark shows significant improvements over the state of the art.",
        "author": "Ming Liang and Bin Yang and Shenlong Wang and Raquel Urtasun",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2012-10992.bib",
        "doi": "10.1007/978-3-030-01270-0_39",
        "journal": "CoRR",
        "keywords": "type: 3D object detection, LiDAR, Autonomous Driving",
        "timestamp": "Mon, 04 Jan 2021 16:33:46 +0100",
        "title": "Deep Continuous Fusion for Multi-Sensor 3D Object Detection",
        "type": "article",
        "url": "https://arxiv.org/abs/2012.10992",
        "volume": "abs/2012.10992",
        "year": "2020"
    },
    "2202.02980": {
        "abstract": "3D object detection from images, one of the fundamental and challenging problems in autonomous driving, has received increasing attention from both industry and academia in recent years. Benefiting from the rapid development of deep learning technologies, image-based 3D detection has achieved remarkable progress. Particularly, more than 200 works have studied this problem from 2015 to 2021, encompassing a broad spectrum of theories, algorithms, and applications. However, to date no recent survey exists to collect and organize this knowledge. In this paper, we fill this gap in the literature and provide the first comprehensive survey of this novel and continuously growing research field, summarizing the most commonly used pipelines for image-based 3D detection and deeply analyzing each of their components. Additionally, we also propose two new taxonomies to organize the state-of-the-art methods into different categories, with the intent of providing a more systematic review of existing methods and facilitating fair comparisons with future works. In retrospect of what has been achieved so far, we also analyze the current challenges in the field and discuss future directions for image-based 3D detection research.",
        "author": "Xinzhu Ma and Wanli Ouyang and Andrea Simonelli and Elisa Ricci",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2202-02980.bib",
        "doi": "10.48550/arXiv.2202.02980",
        "eprint": "2202.02980",
        "eprinttype": "arXiv",
        "journal": "CoRR",
        "keywords": "type: 3D object detection, literature survey",
        "timestamp": "Wed, 09 Feb 2022 15:43:35 +0100",
        "title": "3D Object Detection from Images for Autonomous Driving: {A} Survey",
        "type": "article",
        "url": "https://arxiv.org/abs/2202.02980",
        "volume": "abs/2202.02980",
        "year": "2022"
    },
    "7780463": {
        "abstract": "We focus on the task of amodal 3D object detection in RGB-D images, which aims to produce a 3D bounding box of an object in metric form at its full extent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a 3D volumetric scene from a RGB-D image as input and outputs 3D object bounding boxes. In our approach, we propose the first 3D Region Proposal Network (RPN) to learn objectness from geometric shapes and the first joint Object Recognition Network (ORN) to extract geometric features in 3D and color features in 2D. In particular, we handle objects of various sizes by training an amodal RPN at two different scales and an ORN to regress 3D bounding boxes. Experiments show that our algorithm outperforms the state-of-the-art by 13.8 in mAP and is 200\u00d7 faster than the original Sliding Shapes.",
        "author": "Song, Shuran and Xiao, Jianxiong",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/conf/cvpr/SongX16.bib",
        "booktitle": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
        "doi": "10.1109/CVPR.2016.94",
        "keywords": "type: 3D object detection, RGB-D, indoor",
        "pages": "808-816",
        "publisher": "{IEEE} Computer Society",
        "timestamp": "Fri, 24 Mar 2023 00:02:54 +0100",
        "title": "Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images",
        "type": "INPROCEEDINGS",
        "url": "https://doi.org/10.1109/CVPR.2016.94",
        "year": "2016"
    },
    "8461232": {
        "abstract": "Autonomous driving requires 3D perception of vehicles and other objects in the in environment. Much of the current methods support 2D vehicle detection. This paper proposes a flexible pipeline to adopt any 2D detection network and fuse it with a 3D point cloud to generate 3D information with minimum changes of the 2D detection networks. To identify the 3D box, an effective model fitting algorithm is developed based on generalised car models and score maps. A two-stage convolutional neural network (CNN) is proposed to refine the detected 3D box. This pipeline is tested on the KITTI dataset using two different 2D detection networks. The 3D detection results based on these two networks are similar, demonstrating the flexibility of the proposed pipeline. The results rank second among the 3D detection algorithms, indicating its competencies in 3D detection.",
        "author": "Xinxin Du and Marcelo H. Ang and Sertac Karaman and Daniela Rus",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/conf/icra/DuAKR18.bib",
        "booktitle": "2018 {IEEE} International Conference on Robotics and Automation, {ICRA 2018, Brisbane, Australia, May 21-25, 2018",
        "doi": "10.1109/ICRA.2018.8461232",
        "keywords": "type: 3D object detection, LiDAR, Autonomous Driving",
        "pages": "3194--3200",
        "publisher": "{IEEE}",
        "timestamp": "Wed, 16 Oct 2019 14:14:51 +0200",
        "title": "A General Pipeline for 3D Detection of Vehicles",
        "type": "inproceedings",
        "url": "https://doi.org/10.1109/ICRA.2018.8461232",
        "year": "2018"
    },
    "8578200": {
        "abstract": "In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.",
        "author": "Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/conf/cvpr/QiLWSG18.bib",
        "booktitle": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        "doi": "10.1109/CVPR.2018.00102",
        "keywords": "type: 3D object detection, RGB-D",
        "number": "",
        "pages": "918-927",
        "publisher": "Computer Vision Foundation / {IEEE} Computer Society",
        "timestamp": "Fri, 24 Mar 2023 00:03:00 +0100",
        "title": "Frustum PointNets for 3D Object Detection from RGB-D Data",
        "type": "INPROCEEDINGS",
        "url": "http://openaccess.thecvf.com/content\\_cvpr\\_2018/html/Qi\\_Frustum\\_PointNets\\_for\\_CVPR\\_2018\\_paper.html",
        "volume": "",
        "year": "2018"
    },
    "9093276": {
        "abstract": "Recently, there have been a plethora of classification and detection systems from RGB as well as 3D images. In this work, we describe a new 3D object detection system from an RGB-D or depth-only point cloud. Our system first detects objects in 2D (either RGB, or pseudo-RGB constructed from depth). The next step is to detect 3D objects within the 3D frustums these 2D detections define. This is achieved by voxelizing parts of the frustums (since frustums can be really large), instead of using the whole frustums as done in earlier work. The main novelty of our system has to do with determining which parts (3D proposals) of the frustums to voxelize, thus allowing us to provide high resolution representations around the objects of interest. It also allows our system to have reduced memory requirements. These 3D proposals are fed to an efficient ResNet-based 3D Fully Convolutional Network (FCN). Our 3D detection system is fast, and can be integrated into a robotics platform. With respect to systems that do not perform voxelization (such as PointNet), our methods can operate without the requirement of subsampling of the datasets. We have also introduced a pipelining approach that further improves the efficiency of our system. Results on SUN RGB-D dataset show that our system, which is based on a small network, can process 20 frames per second with comparable detection results to the state-of-the-art [16], achieving a 2x speedup.",
        "author": "Shen, Xiaoke and Stamos, Ioannis",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/conf/wacv/ShenS20.bib",
        "booktitle": "{IEEE} Winter Conference on Applications of Computer Vision, {WACV 2020, Snowmass Village, CO, USA, March 1-5, 2020",
        "doi": "10.1109/WACV45572.2020.9093276",
        "keywords": "type: 3D object detection, RGB-D",
        "pages": "1687--1695",
        "publisher": "{IEEE}",
        "timestamp": "Tue, 21 Mar 2023 20:55:11 +0100",
        "title": "Frustum VoxNet for 3D object detection from {RGB-D} or Depth images",
        "type": "inproceedings",
        "url": "https://doi.org/10.1109/WACV45572.2020.9093276",
        "year": "2020"
    },
    "9196660": {
        "abstract": "Safe autonomous driving requires reliable 3D object detection-determining the 6 DoF pose and dimensions of objects of interest. Using stereo cameras to solve this task is a cost-effective alternative to the widely used LiDAR sensor. The current state-of-the-art for stereo 3D object detection takes the existing PSMNet stereo matching network, with no modifications, and converts the estimated disparities into a 3D point cloud, and feeds this point cloud into a LiDAR-based 3D object detector. The issue with existing stereo matching networks is that they are designed for disparity estimation, not 3D object detection; the shape and accuracy of object point clouds are not the focus. Stereo matching networks commonly suffer from inaccurate depth estimates at object boundaries, which we define as streaking, because background and foreground points are jointly estimated. Existing networks also penalize disparity instead of the estimated position of object point clouds in their loss functions. We propose a novel 2D box association and object-centric stereo matching method that only estimates the disparities of the objects of interest to address these two issues. Our method achieves state-of-the-art results on the KITTI 3D and BEV benchmarks.",
        "author": "Alex D. Pon and Jason Ku and Chengyao Li and Steven L. Waslander",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/conf/icra/PonKLW20.bib",
        "booktitle": "2020 {IEEE} International Conference on Robotics and Automation, {ICRA 2020, Paris, France, May 31 - August 31, 2020",
        "doi": "10.1109/ICRA40945.2020.9196660",
        "keywords": "type: 3D object detection, binocular, Autonomous Driving",
        "pages": "8383--8389",
        "publisher": "{IEEE}",
        "timestamp": "Mon, 03 May 2021 18:50:35 +0200",
        "title": "Object-Centric Stereo Matching for 3D Object Detection",
        "type": "inproceedings",
        "url": "https://doi.org/10.1109/ICRA40945.2020.9196660",
        "year": "2020"
    },
    "9461875": {
        "abstract": "Many research works focus on leveraging the complementary geometric information of indoor depth sensors in vision tasks performed by deep convolutional neural networks, notably semantic segmentation. These works deal with a specific vision task known as \"RGB-D Indoor Semantic Segmentation\". The challenges and resulting solutions of this task differ from its standard RGB counterpart. This results in a new active research topic. The objective of this paper is to introduce the field of Deep Convolutional Neural Networks for RGB-D Indoor Semantic Segmentation. This review presents the most popular public datasets, proposes a categorization of the strategies employed by recent contributions, evaluates the performance of the current state-of-the-art, and discusses the remaining challenges and promising directions for future works.",
        "author": "Sami Barchid and Jos{\\'{e}} Mennesson and Chaabane Djeraba",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/conf/cbmi/BarchidMD21.bib",
        "booktitle": "18th International Conference on Content-Based Multimedia Indexing, {CBMI} 2021, Lille, France, June 28-30, 2021",
        "doi": "10.1109/CBMI50038.2021.9461875",
        "keywords": "type: 3D object detection, RGB-D, indoor",
        "pages": "1--4",
        "publisher": "{IEEE}",
        "timestamp": "Tue, 29 Jun 2021 17:37:02 +0200",
        "title": "Review on Indoor {RGB-D} Semantic Segmentation with Deep Convolutional Neural Networks",
        "type": "inproceedings",
        "url": "https://doi.org/10.1109/CBMI50038.2021.9461875",
        "year": "2021"
    },
    "9561675": {
        "abstract": "Analyzing scenes thoroughly is crucial for mobile robots acting in different environments. Semantic segmentation can enhance various subsequent tasks, such as (semantically assisted) person perception, (semantic) free space detection, (semantic) mapping, and (semantic) navigation. In this paper, we propose an efficient and robust RGB-D segmentation approach that can be optimized to a high degree using NVIDIA TensorRT and, thus, is well suited as a common initial processing step in a complex system for scene analysis on mobile robots. We show that RGB-D segmentation is superior to processing RGB images solely and that it can still be performed in real time if the network architecture is carefully designed. We evaluate our proposed Efficient Scene Analysis Network (ESANet) on the common indoor datasets NYUv2 and SUNRGB-D and show that we reach state-of-the-art performance while enabling faster inference. Furthermore, our evaluation on the outdoor dataset Cityscapes shows that our approach is suitable for other areas of application as well. Finally, instead of presenting benchmark results only, we also show qualitative results in one of our indoor application scenarios.",
        "author": "Daniel Seichter and Mona K{\\\"{o}}hler and Benjamin Lewandowski and Tim Wengefeld and Horst{-}Michael Gross",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/conf/icra/SeichterKLWG21.bib",
        "booktitle": "{IEEE} International Conference on Robotics and Automation, {ICRA 2021, Xi'an, China, May 30 - June 5, 2021",
        "doi": "10.1109/ICRA48506.2021.9561675",
        "keywords": "type: 3D object detection, RGB-D, indoor",
        "pages": "13525--13531",
        "publisher": "{IEEE}",
        "timestamp": "Mon, 25 Oct 2021 11:20:08 +0200",
        "title": "Efficient {RGB-D} Semantic Segmentation for Indoor Scene Analysis",
        "type": "inproceedings",
        "url": "https://doi.org/10.1109/ICRA48506.2021.9561675",
        "year": "2021"
    }
};